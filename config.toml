# Multi-Agent Competition System Configuration
# See .env.example for required environment variables

[database]
path = "agent_leaderboard.duckdb"

[execution]
timeout_seconds = 60

[[task_agents]]
provider = "gemini"
model = "gemini-2.0-flash-lite"
api_key_env = "GOOGLE_API_KEY"

[[task_agents]]
provider = "gemini"
model = "gemini-2.0-flash"
api_key_env = "GOOGLE_API_KEY"

[evaluation_agent]
# Note: "gemini" is automatically mapped to "google-gla:" provider internally
provider = "gemini"
model = "gemini-2.5-pro"
api_key_env = "GOOGLE_API_KEY"
prompt = """You are evaluating the performance of an AI agent that completed a task.

Task: {task_prompt}

Agent's Response: {agent_response}

Please evaluate the agent's response on the following criteria:
1. Correctness: Did the agent correctly complete the task?
2. Efficiency: Did the agent use tools appropriately?
3. Completeness: Did the agent provide a complete answer?

Provide:
1. A score from 0-100 (where 100 is perfect)
2. A brief explanation of your evaluation

Response format:
Score: [number]
Explanation: [your explanation]"""
