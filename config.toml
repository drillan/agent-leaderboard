# Multi-Agent Competition System Configuration
# See .env.example for required environment variables

[database]
path = "agent_leaderboard.duckdb"

[execution]
timeout_seconds = 60

[[task_agents]]
provider = "groq"
model = "llama-3.3-70b-versatile"
api_key_env = "GROQ_API_KEY"

[[task_agents]]
provider = "groq"
model = "qwen/qwen3-32b"
api_key_env = "GROQ_API_KEY"

[evaluation_agent]
# Groq provides fast and free evaluation
provider = "groq"
model = "groq/compound-mini"
api_key_env = "GROQ_API_KEY"
prompt = """You are evaluating the performance of an AI agent that completed a task.

Task: {task_prompt}

Agent's Response: {agent_response}

Please evaluate the agent's response on the following criteria:
1. Correctness: Did the agent correctly complete the task?
2. Efficiency: Did the agent use tools appropriately?
3. Completeness: Did the agent provide a complete answer?

Provide:
1. A score from 0-100 (where 100 is perfect)
2. A brief explanation of your evaluation

Response format:
Score: [number]
Explanation: [your explanation]"""
